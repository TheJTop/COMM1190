This is my current code: from llama_index.core import Document, VectorStoreIndex

documents = [
    Document(
        text=row['text'],
        metadata={'document_name': row['metadata'], 'region': row['region']},
        embedding=row['embedding']
    )
    for _, row in embeddings_df.iterrows()
]

index = VectorStoreIndex.from_documents(documents)... I am using llama index - im getting the error that says 
Could not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.
Original error:
No API key found for OpenAI.
Please set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.
API keys can be found or created at https://platform.openai.com/account/api-keys
 - however i have already embedded by documents - how can i put them into an index without needing to embeded them again?
